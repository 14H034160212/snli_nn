import numpy as np
import theano
import theano.tensor as T

def vanilla(params, gradients, learning_rate):
    return [(param, param - learning_rate * gradient) 
            for param, gradient in zip(params, gradients)]

#def sparse_vanilla(params, sub_params, gradients, learning_rate):
#    return [(param, T.inc_subtensor(sub_param, -learning_rate * gradient)) 
#            for param, sub_param, gradient in zip(params, sub_params, gradients)]

def rmsprop(params, gradients, learning_rate):
    updates = []
    for param_t0, gradient in zip(params, gradients):
        # rmsprop see slide 29 of http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
        # first the mean_sqr exponential moving average
        mean_sqr_t0 = theano.shared(np.zeros(param_t0.get_value().shape, dtype=param_t0.get_value().dtype))  # zeros in same shape are param
        mean_sqr_t1 = 0.9 * mean_sqr_t0 + 0.1 * gradient**2
        updates.append((mean_sqr_t0, mean_sqr_t1))
        # update param surpressing gradient by this average
        param_t1 = param_t0 - learning_rate * (gradient / T.sqrt(mean_sqr_t1 + 1e-10))
        updates.append((param_t0, param_t1))
    return updates
